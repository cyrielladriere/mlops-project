# PIPELINE DEFINITION
# Name: eval-model
components:
  comp-eval-model:
    executorLabel: exec-eval-model
deploymentSpec:
  executors:
    exec-eval-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - eval_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef eval_model() -> None:\n    \"\"\"Predict news category for news\
          \ articles using the trained model.\"\"\"\n    dataloader = torch.load(DATALOADER_LOC)\n\
          \n    device = torch.device(\"cuda\") if torch.cuda.is_available() else\
          \ torch.device(\"cpu\")\n\n    label_encoder = get_label_encoder()\n   \
          \ n_classes = len(label_encoder.classes_)\n\n    model = BertForSequenceClassification.from_pretrained(\n\
          \        \"bert-base-uncased\", num_labels=n_classes\n    )\n    model.load_state_dict(torch.load(MODEL_LOC))\n\
          \n    model.eval()\n    all_preds = []\n    all_labels = []\n    for i,\
          \ batch in enumerate(dataloader):\n        print(f\"[Eval] batch: {i}/{len(dataloader)}\"\
          )\n        batch = {k: v.to(device) for k, v in batch.items()}\n       \
          \ with torch.no_grad():\n            outputs = model(\n                input_ids=batch[\"\
          input_ids\"], attention_mask=batch[\"attention_mask\"]\n            )\n\n\
          \        preds = outputs.logits.detach().cpu().numpy()\n        all_preds.append(preds)\n\
          \n        labels = batch[\"label\"].float().detach().cpu().numpy()\n   \
          \     all_labels.append(labels)\n\n    all_preds = np.concatenate(all_preds,\
          \ axis=0)\n    all_labels = np.concatenate(all_labels, axis=0)\n\n    metrics\
          \ = compute_metrics(all_preds, all_labels)\n    print(\"Performance model\
          \ on test set: \", metrics)\n\n    all_preds = np.argmax(all_preds, axis=1)\n\
          \    print(label_encoder.inverse_transform(all_preds))\n\n"
        image: python:3.10
pipelineInfo:
  name: eval-model
root:
  dag:
    tasks:
      eval-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-eval-model
        taskInfo:
          name: eval-model
schemaVersion: 2.1.0
sdkVersion: kfp-2.8.0
